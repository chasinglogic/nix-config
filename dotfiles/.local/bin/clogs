#!/usr/bin/env python3

import asyncio
import sys
import argparse
import os
import itertools

try:
    import httpx
except ImportError:
    print("You must install httpx.")
    sys.exit(1)


# List of possible values here:
# https://circleci.com/docs/api/v2/index.html#operation/getJobDetails
IN_PROGRESS_STATUSES = [
        "blocked",
    "queued",
    "running",
]

TOKEN = os.getenv("CIRCLECI_TOKEN", "")
NO_COLOR = os.getenv("NO_COLOR")
CLIENT = httpx.AsyncClient(follow_redirects=True, auth=(TOKEN, ""))
PROJECT_SLUG = None

RESET_COLOR = "\u001b[0m"
JOB_COLORS = itertools.cycle(
    # Generate all 231 color escape codes excluding the hard to read ones
    #
    # Everything after 232 is illegible so only go up to 232 instead of 255
    [
        f"\u001b[38;5;{color}m"
        for color in range(232)
        if color
        not in [
            # Grays.
            0,
            8,
            59,
            60,
            61,
            101,
            102,
            103,
            # Blacks.
            16,
            17,
            # Dark Blues.
            18,
            19,
            20,
        ]
    ]
)


async def get_pipeline(pipeline_number):
    res = await CLIENT.get(
        f"https://circleci.com/api/v2/project/{PROJECT_SLUG}/pipeline/{pipeline_number}",
    )
    res.raise_for_status()
    return res.json()


async def get_workflows(pipeline):
    pipeline_id = pipeline["id"]
    res = await CLIENT.get(
        f"https://circleci.com/api/v2/pipeline/{pipeline_id}/workflow",
    )
    res.raise_for_status()
    return res.json()["items"]


async def get_jobs(workflow=None, workflow_id=None):
    assert workflow is not None or workflow_id is not None

    if workflow_id is None and workflow is not None:
        workflow_id = workflow["id"]

    res = await CLIENT.get(
        f"https://circleci.com/api/v2/workflow/{workflow_id}/job",
    )
    res.raise_for_status()

    data = res.json()

    return data["items"]


async def get_job(job_number):
    res = await CLIENT.get(
        f"https://circleci.com/api/v1.1/project/{PROJECT_SLUG}/{job_number}",
    )
    res.raise_for_status()
    job = res.json()
    # When the Job is queued (i.e. hasn't started) this API does not return the
    # job number in the data even though the job has one. All the other polling
    # in this app expects job_number to always be a populated field so since we
    # have the number fill it in if Circle didn't give us one.
    if not job.get("job_number"):
        job["job_number"] = job_number
    return job


async def get_job_by_id(workflow_id, job_id):
    jobs = await get_jobs(workflow_id=workflow_id)
    for job in jobs:
        if job['id'] == job_id:
            return job

    assert False, f"Job {job_id} not found in workflow {workflow_id}"

def parse_pipeline_url(url):
    split = url.split("/")
    return (
        "/".join(split[-4:-1]),
        # Pipeline number is always last element.
        split[-1],
    )


async def print_all_step_logs(job, job_name, skip_list=None):
    printed_ids = []
    if skip_list is None:
        skip_list = []

    for step in job["steps"]:
        for action in step["actions"]:
            if action.get("allocation_id") in skip_list:
                continue

            output_url = action.get("output_url")
            # This just means it doesn't have output yet.
            if not output_url:
                continue

            res = await CLIENT.get(output_url, auth=None)
            res.raise_for_status()
            data = res.json()
            for d in data:
                msg = d["message"].split("\n")
                for line in msg:
                    print(job_name, "|", RESET_COLOR, line)

            printed_ids.append(action["allocation_id"])

    return printed_ids


async def print_logs(
    job,
    live_stream=True,
):
    job_name = job["workflows"]["job_name"]
    if not NO_COLOR:
        color = next(JOB_COLORS)
        job_name = f"{color}{job_name}"

    if not live_stream:
        await print_all_step_logs(job, job_name)
        return

    printed = []
    while live_stream and job["status"] in IN_PROGRESS_STATUSES:
        printed.extend(await print_all_step_logs(job, job_name, skip_list=printed))
        await asyncio.sleep(3)
        job = await get_job(job["job_number"])

    # Print any left over logs since the last time we polled.
    await print_all_step_logs(job, job_name, skip_list=printed)

async def print_completed_logs(jobs):
    print('Printing logs...')
    for job in jobs:
        if 'job_number' not in job:
            print('Skipping job since it has no number:', job.get('job_name', 'UNKNOWN'))
            continue

        job = await get_job(job['job_number'])
        await print_logs(job, live_stream=False)

async def live_stream_logs(workflows):
    print('Live streaming logs...')
    for workflow in workflows:
        jobs = await get_jobs(workflow)
        for job in jobs:
            while "job_number" not in job:
                job = await get_job_by_id(workflow['id'], job['id'])

            job = await get_job(job["job_number"])
            await print_logs(job)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-s",
        "--slug",
        default=None,
        help="CircleCI project slug of the pipeline of the form "
        "{vcs_name}/{org_name}/{project_name}. If --url is used will be parsed"
        " from that, otherwise it is required.",
    ),
    parser.add_argument(
        "-u",
        "--url",
        default=None,
        type=str,
        help="A URL to the pipeline which you want to stream logs for.",
    )
    parser.add_argument(
        "-p",
        "--pipeline",
        default=None,
        type=int,
        help="The pipeline number which you want to stream logs for.",
    )
    return parser.parse_args()

async def main():
    global PROJECT_SLUG

    if not TOKEN:
        print("You must set $CIRCLECI_TOKEN to use this tool!")
        sys.exit(1)

    args = parse_args()
    assert args.url or args.pipeline, "Must provide either --url or --pipeline"

    if args.url:
        PROJECT_SLUG, pipeline_number = parse_pipeline_url(args.url)
    else:
        PROJECT_SLUG = args.slug
        pipeline_number = args.pipeline

    assert PROJECT_SLUG, "You must either provide --slug or --url"

    print('Retrieving pipeline')
    pipeline = await get_pipeline(pipeline_number)

    print('Retrieving workflows')
    workflows = await get_workflows(pipeline)

    print('Retrieving jobs')
    all_jobs = [
        job
        for workflow in workflows
        for job in await get_jobs(workflow)
    ]
    already_done = all(job["status"] not in IN_PROGRESS_STATUSES for job in all_jobs)
    if already_done:
        await print_completed_logs(all_jobs)
    else:
        await live_stream_logs(workflows)

if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
