#!/usr/bin/env python3

import pprint
import sys
import argparse
import os
import itertools

from enum import Enum

try:
    import httpx
except ImportError:
    print("You must install httpx.")
    sys.exit(1)


class JobStatus(Enum):
    SUCCESS = "success"
    RUNNING = "running"
    NOT_RUN = "not_run"
    FAILED = "failed"
    RETRIED = "retried"
    QUEUED = "queued"
    NOT_RUNNING = "not_running"
    INFRASTRUCTURE_FAIL = "infrastructure_fail"
    TIMEDOUT = "timedout"
    ON_HOLD = "on_hold"
    TERMINATED_UNKNOWN = "terminated-unknown"
    BLOCKED = "blocked"
    CANCELED = "canceled"
    UNAUTHORIZED = "unauthorized"


IN_PROGRESS_STATUSES = [
    JobStatus.QUEUED.value,
    JobStatus.RUNNING.value,
]

TOKEN = os.getenv("CIRCLECI_TOKEN", "")
NO_COLOR = os.getenv("NO_COLOR")
CLIENT = httpx.AsyncClient(follow_redirects=True, auth=(TOKEN, ""))


class Foreground(Enum):
    black = "\u001b[30m"
    red = "\u001b[31m"
    green = "\u001b[32m"
    yellow = "\u001b[33m"
    blue = "\u001b[34m"
    magenta = "\u001b[35m"
    cyan = "\u001b[36m"


JOB_COLORS = itertools.cycle(
    # Generate all 231 color escape codes excluding the hard to read ones
    #
    # Everything after 232 is illegible so only go up to 232 instead of 255
    [
        f"\u001b[38;5;{color}m"
        for color in range(232)
        if color
        not in [
            # Grays.
            0,
            8,
            59,
            60,
            61,
            101,
            102,
            103,
            # Blacks.
            16,
            17,
            # Dark Blues.
            18,
            19,
            20,
        ]
    ]
)

RESET_COLOR = "\u001b[0m"
PROJECT_SLUG = None


async def get_pipeline(pipeline_number):
    res = await CLIENT.get(
        f"https://circleci.com/api/v2/project/{PROJECT_SLUG}/pipeline/{pipeline_number}",
    )
    res.raise_for_status()

    return res.json()


async def get_workflows(pipeline):
    pipeline_id = pipeline["id"]
    res = await CLIENT.get(
        f"https://circleci.com/api/v2/pipeline/{pipeline_id}/workflow",
    )
    res.raise_for_status()

    return res.json()["items"]


async def get_jobs(workflow=None, workflow_id=None):
    assert workflow is not None or workflow_id is not None

    if workflow_id is None and workflow is not None:
        workflow_id = workflow["id"]

    data = None

    # Have to check for every job to have a number, this can be false if the
    # script is started shortly after the pipeline is triggered.
    while data is None or not all("job_number" in job for job in data["items"]):
        res = await CLIENT.get(
            f"https://circleci.com/api/v2/workflow/{workflow_id}/job",
        )
        res.raise_for_status()

        data = res.json()

    return data["items"]


async def get_job(job_number):
    res = await CLIENT.get(
        f"https://circleci.com/api/v1.1/project/{PROJECT_SLUG}/{job_number}",
    )
    res.raise_for_status()
    job = res.json()
    # When the Job is queued (i.e. hasn't started) this API does not return the
    # job number in the data even though the job has one. All the other polling
    # in this app expects job_number to always be a populated field so since we
    # have the number fill it in if Circle didn't give us one.
    if not job.get("job_number"):
        job["job_number"] = job_number
    return job


def parse_pipeline_url(url):
    split = url.split("/")
    return (
        "/".join(split[-4:-1]),
        # Pipeline number is always last element.
        split[-1],
    )


async def print_all_step_logs(job, job_name, skip_list=None):
    printed_ids = []
    if skip_list is None:
        skip_list = []

    for step in job["steps"]:
        for action in step["actions"]:
            if action.get("allocation_id") in skip_list:
                continue

            output_url = action.get("output_url")
            # This just means it doesn't have output yet.
            if not output_url:
                continue

            res = await CLIENT.get(output_url, auth=None)
            res.raise_for_status()
            data = res.json()
            for d in data:
                msg = d["message"].split("\n")
                for line in msg:
                    print(job_name, "|", RESET_COLOR, line)

            printed_ids.append(action["allocation_id"])

    return printed_ids


async def print_logs(
    job,
    live_stream=True,
):
    job_name = job["workflows"]["job_name"]
    if not NO_COLOR:
        color = next(JOB_COLORS)
        job_name = f"{color}{job_name}"

    if not live_stream:
        await print_all_step_logs(job, job_name)
        return

    printed = []
    while live_stream and job["status"] in IN_PROGRESS_STATUSES:
        printed.extend(await print_all_step_logs(job, job_name, skip_list=printed))
        job = await get_job(job["job_number"])


async def main():
    global PROJECT_SLUG

    if not TOKEN:
        print("You must set $CIRCLECI_TOKEN to use this tool!")
        sys.exit(1)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-s",
        "--slug",
        default=None,
        help="CircleCI project slug of the pipeline of the form "
        "{vcs_name}/{org_name}/{project_name}. If --url is used will be parsed"
        " from that, otherwise it is required.",
    ),
    parser.add_argument(
        "-u",
        "--url",
        default=None,
        type=str,
        help="A URL to the pipeline which you want to stream logs for.",
    )
    parser.add_argument(
        "-p",
        "--pipeline",
        default=None,
        type=int,
        help="The pipeline number which you want to stream logs for.",
    )
    args = parser.parse_args()

    assert args.url or args.pipeline, "Must provide either --url or --pipeline"

    if args.url:
        PROJECT_SLUG, pipeline_number = parse_pipeline_url(args.url)
    else:
        PROJECT_SLUG = args.slug
        pipeline_number = args.pipeline

    assert PROJECT_SLUG, "You must either provide --slug or --url"
    pipeline = await get_pipeline(pipeline_number)
    workflows = await get_workflows(pipeline)

    jobs = [
        await get_job(job["job_number"])
        for workflow in workflows
        for job in await get_jobs(workflow)
    ]
    already_done = all(job["status"] not in IN_PROGRESS_STATUSES for job in jobs)

    for job in jobs:
        await print_logs(job, live_stream=not already_done)


if __name__ == "__main__":
    import asyncio

    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
